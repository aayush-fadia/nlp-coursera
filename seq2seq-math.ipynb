{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to calculate with seq2seq model\n",
    "\n",
    "In this assignment, you will learn how to use neural networks to solve sequence-to-sequence prediction tasks. Seq2Seq models are very popular these days because they achieve great results in Machine Translation, Text Summarization, Conversational Modeling and more.\n",
    "\n",
    "Using sequence-to-sequence modeling you are going to build a calculator for evaluating arithmetic expressions, by taking an equation as an input to the neural network and producing an answer as it's output.\n",
    "\n",
    "The resulting solution for this problem will be based on state-of-the-art approaches for sequence-to-sequence learning and you should be able to easily adapt it to solve other tasks. However, if you want to train your own machine translation system or intellectual chat bot, it would be useful to have access to compute resources like GPU, and be patient, because training of such systems is usually time consuming. \n",
    "\n",
    "### Libraries\n",
    "\n",
    "For this task you will need the following libraries:\n",
    " - [TensorFlow](https://www.tensorflow.org) — an open-source software library for Machine Intelligence.\n",
    " \n",
    "In this assignment, we use Tensorflow 1.15.0. You can install it with pip:\n",
    "\n",
    "    !pip install tensorflow==1.15.0\n",
    "     \n",
    " - [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    " \n",
    "If you have never worked with TensorFlow, you will probably want to read some tutorials during your work on this assignment, e.g. [Neural Machine Translation](https://www.tensorflow.org/tutorials/seq2seq) tutorial deals with very similar task and can explain some concepts to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    ! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
    "    import setup_google_colab\n",
    "    setup_google_colab.setup_week4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "One benefit of this task is that you don't need to download any data — you will generate it on your own! We will use two operators (addition and subtraction) and work with positive integer numbers in some range. Here are examples of correct inputs and outputs:\n",
    "\n",
    "    Input: '1+2'\n",
    "    Output: '3'\n",
    "    \n",
    "    Input: '0-99'\n",
    "    Output: '-99'\n",
    "\n",
    "*Note, that there are no spaces between operators and operands.*\n",
    "\n",
    "\n",
    "Now you need to implement the function *generate_equations*, which will be used to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_equations(allowed_operators, dataset_size, min_value, max_value):\n",
    "    \"\"\"Generates pairs of equations and solutions to them.\n",
    "    \n",
    "       Each equation has a form of two integers with an operator in between.\n",
    "       Each solution is an integer with the result of the operaion.\n",
    "    \n",
    "        allowed_operators: list of strings, allowed operators.\n",
    "        dataset_size: an integer, number of equations to be generated.\n",
    "        min_value: an integer, min value of each operand.\n",
    "        max_value: an integer, max value of each operand.\n",
    "\n",
    "        result: a list of tuples of strings (equation, solution).\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    op1s = random.choices(range(min_value, max_value), k=dataset_size)\n",
    "    op2s = random.choices(range(min_value, max_value), k=dataset_size)\n",
    "    for x, y in zip(op1s, op2s):\n",
    "        operator = random.choice(allowed_operators)\n",
    "        question = str(x)+operator+str(y)\n",
    "        sample.append((question, str(eval(question))))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the correctness of your implementation, use *test_generate_equations* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_equations():\n",
    "    allowed_operators = ['+', '-']\n",
    "    dataset_size = 10\n",
    "    for (input_, output_) in generate_equations(allowed_operators, dataset_size, 0, 100):\n",
    "        if not (type(input_) is str and type(output_) is str):\n",
    "            return \"Both parts should be strings.\"\n",
    "        if eval(input_) != int(output_):\n",
    "            return \"The (equation: {!r}, solution: {!r}) pair is incorrect.\".format(input_, output_)\n",
    "    return \"Tests passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_generate_equations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to generate the train and test data for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_operators = ['+', '-']\n",
    "dataset_size = 100000\n",
    "data = generate_equations(allowed_operators, dataset_size, min_value=0, max_value=9999)\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for the neural network\n",
    "\n",
    "The next stage of data preparation is creating mappings of the characters to their indices in some vocabulary. Since in our task we already know which symbols will appear in the inputs and outputs, generating the vocabulary is a simple step.\n",
    "\n",
    "#### How to create dictionaries for other task\n",
    "\n",
    "First of all, you need to understand what is the basic unit of the sequence in your task. In our case, we operate on symbols and the basic unit is a symbol. The number of symbols is small, so we don't need to think about filtering/normalization steps. However, in other tasks, the basic unit is often a word, and in this case the mapping would be *word $\\to$ integer*. The number of words might be huge, so it would be reasonable to filter them, for example, by frequency and leave only the frequent ones. Other strategies that your should consider are: data normalization (lowercasing, tokenization, how to consider punctuation marks), separate vocabulary for input and for output (e.g. for machine translation), some specifics of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {symbol:i for i, symbol in enumerate('#^$+-1234567890')}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = '^'\n",
    "end_symbol = '$'\n",
    "padding_symbol = '#'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could notice that we have added 3 special symbols: '^', '\\$' and '#':\n",
    "- '^' symbol will be passed to the network to indicate the beginning of the decoding procedure. We will discuss this one later in more details.\n",
    "- '\\$' symbol will be used to indicate the *end of a string*, both for input and output sequences. \n",
    "- '#' symbol will be used as a *padding* character to make lengths of all strings equal within one training batch.\n",
    "\n",
    "People have a bit different habits when it comes to special symbols in encoder-decoder networks, so don't get too much confused if you come across other variants in tutorials you read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When vocabularies are ready, we need to be able to convert a sentence to a list of vocabulary word indices and back. At the same time, let's care about padding. We are going to preprocess each sequence from the input (and output ground truth) in such a way that:\n",
    "- it has a predefined length *padded_len*\n",
    "- it is probably cut off or padded with the *padding symbol* '#'\n",
    "- it *always* ends with the *end symbol* '$'\n",
    "\n",
    "We will treat the original characters of the sequence **and the end symbol** as the valid part of the input. We will store *the actual length* of the sequence, which includes the end symbol, but does not include the padding symbols. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now you need to implement the function *sentence_to_ids* that does the described job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(sentence, word2id, padded_len):\n",
    "    \"\"\" Converts a sequence of symbols to a padded sequence of their ids.\n",
    "    \n",
    "      sentence: a string, input/output sequence of symbols.\n",
    "      word2id: a dict, a mapping from original symbols to ids.\n",
    "      padded_len: an integer, a desirable length of the sequence.\n",
    "\n",
    "      result: a tuple of (a list of ids, an actual length of sentence).\n",
    "    \"\"\"\n",
    "    sent_ids = [word2id[x] for x in sentence]\n",
    "    if(len(sent_ids) >= padded_len):\n",
    "        sent_ids = sent_ids[:padded_len]\n",
    "        sent_ids[-1] = word2id['$']\n",
    "        sent_len = len(sent_ids)\n",
    "    else:\n",
    "        sent_ids.append(word2id['$'])\n",
    "        sent_len = len(sent_ids)\n",
    "        sent_ids += [word2id['#']]*(padded_len-len(sent_ids))\n",
    "    return sent_ids, sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_to_ids():\n",
    "    sentences = [(\"123+123\", 7), (\"123+123\", 8), (\"123+123\", 10)]\n",
    "    expected_output = [([5, 6, 7, 3, 5, 6, 2], 7), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 2], 8), \n",
    "                       ([5, 6, 7, 3, 5, 6, 7, 2, 0, 0], 8)] \n",
    "    for (sentence, padded_len), (sentence_ids, expected_length) in zip(sentences, expected_output):\n",
    "        output, length = sentence_to_ids(sentence, word2id, padded_len)\n",
    "        if output != sentence_ids:\n",
    "            return(\"Convertion of '{}' for padded_len={} to {} is incorrect.\".format(\n",
    "                sentence, padded_len, output))\n",
    "        if length != expected_length:\n",
    "            return(\"Convertion of '{}' for padded_len={} has incorrect actual length {}.\".format(\n",
    "                sentence, padded_len, length))\n",
    "    return(\"Tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence_to_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to get back from indices to symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, id2word):\n",
    "    \"\"\" Converts a sequence of ids to a sequence of symbols.\n",
    "    \n",
    "          ids: a list, indices for the padded sequence.\n",
    "          id2word:  a dict, a mapping from ids to original symbols.\n",
    "\n",
    "          result: a list of symbols.\n",
    "    \"\"\"\n",
    " \n",
    "    return [id2word[i] for i in ids] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of data preparation is a function that transforms a batch of sentences to a list of lists of indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    \"\"\"Prepares batches of indices. \n",
    "    \n",
    "       Sequences are padded to match the longest sequence in the batch,\n",
    "       if it's longer than max_len, then max_len is used instead.\n",
    "\n",
    "        sentences: a list of strings, original sequences.\n",
    "        word2id: a dict, a mapping from original symbols to ids.\n",
    "        max_len: an integer, max len of sequences allowed.\n",
    "\n",
    "        result: a list of lists of ids, a list of actual lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *generate_batches* will help to generate batches with defined size from given samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(samples, batch_size=64):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the result of the implemented functions, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ('7200-4366', '2834')\n",
      "Ids: [[11, 6, 14, 14, 4, 8, 7, 10, 10, 2], [6, 12, 7, 8, 2, 0, 0, 0, 0, 0]]\n",
      "Sentences lengths: [10, 5]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_set[0]\n",
    "ids, sent_lens = batch_to_ids(sentences, word2id, max_len=10)\n",
    "print('Input:', sentences)\n",
    "print('Ids: {}\\nSentences lengths: {}'.format(ids, sent_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder architecture\n",
    "\n",
    "Encoder-Decoder is a successful architecture for Seq2Seq tasks with different lengths of input and output sequences. The main idea is to use two recurrent neural networks, where the first neural network *encodes* the input sequence into a real-valued vector and then the second neural network *decodes* this vector into the output sequence. While building the neural network, we will specify some particular characteristics of this architecture.\n",
    "\n",
    "# Modified Section\n",
    "## Tensorflow 2.x is used, instead of 1.x, which does away with all the session and placeholder mechanics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"encoder-decoder-pic.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(1)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "n_epochs = 32\n",
    "dropout_drop_probability = 0.25\n",
    "max_len = 20\n",
    "recurrent_dims = 512\n",
    "common_embedding = Embedding(len(word2id), 16)\n",
    "learning_rate = 0.005\n",
    "max_decoder_steps = 7\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = common_embedding\n",
    "        self.gru = GRU(recurrent_dims)\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        embeddeds = self.embedding(inputs)\n",
    "        outs = self.gru(embeddeds, )\n",
    "        if training:\n",
    "            outs = self.dropout(outs)\n",
    "        return outs\n",
    "\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = common_embedding\n",
    "        self.concatter = Concatenate()\n",
    "        self.gru = GRU(recurrent_dims, return_sequences=True, return_state=True)\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "        self.head = Dense(len(word2id), activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        sequence, thought_vector, hidden_states = inputs\n",
    "        embeddeds = self.embedding(sequence)\n",
    "        concatted = self.concatter([embeddeds, thought_vector])\n",
    "        gru_outs, hidden_state = self.gru(concatted, initial_state=hidden_states)\n",
    "        if training:\n",
    "            gru_outs = self.dropout(gru_outs)\n",
    "        final_outs = self.head(gru_outs)\n",
    "        return final_outs, hidden_state\n",
    "\n",
    "\n",
    "\n",
    "encoder = Encoder(dropout_drop_probability)\n",
    "decoder = Decoder(dropout_drop_probability)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "loss_op = CategoricalCrossentropy(reduction='none')\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    mask = tf.not_equal(targets, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    targets = to_categorical(targets, len(word2id), dtype=np.float32)\n",
    "    loss_ = loss_op(targets, outputs)\n",
    "    loss_ = loss_ * mask\n",
    "    loss = tf.reduce_mean(loss_)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(x_batch, y_batch):\n",
    "    new_column = np.asarray([word2id['^']] * y_batch.shape[0])\n",
    "    y_batch_in = np.insert(y_batch, 0, new_column, 1)[:, :-1]\n",
    "    with tf.GradientTape() as tape:\n",
    "        thought_vectors = encoder(x_batch, training=True)\n",
    "        thought_vectors = tf.expand_dims(thought_vectors, 1)\n",
    "        thought_vectors = tf.repeat(thought_vectors, y_batch.shape[1], 1)\n",
    "        outs, _ = decoder((y_batch_in, thought_vectors, np.zeros((y_batch.shape[0], recurrent_dims))), training=True)\n",
    "        loss = loss_fn(outs, targets=y_batch)\n",
    "    all_variables = common_embedding.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "    grads = tape.gradient(loss, all_variables)\n",
    "    optimizer.apply_gradients(zip(grads, all_variables))\n",
    "\n",
    "\n",
    "def predict(x_batch):\n",
    "    thought_vectors = encoder(x_batch)\n",
    "    hidden_state = np.zeros((x_batch.shape[0], recurrent_dims))\n",
    "    thought_vectors = tf.expand_dims(thought_vectors, 1)\n",
    "    generated_ids = [np.asarray([[1]] * x_batch.shape[0])]\n",
    "    while len(generated_ids) < max_decoder_steps:\n",
    "        outs, hidden_state = decoder((generated_ids[-1], thought_vectors, hidden_state))\n",
    "        generated_ids.append(tf.argmax(outs, -1))\n",
    "    return np.asarray(generated_ids)[:, :, 0].T[:, 1:]\n",
    "\n",
    "\n",
    "def predict_with_loss(x_batch, y_batch):\n",
    "    thought_vectors = encoder(x_batch)\n",
    "    hidden_state = np.zeros((x_batch.shape[0], recurrent_dims))\n",
    "    thought_vectors = tf.expand_dims(thought_vectors, 1)\n",
    "    generated_ids = [np.asarray([[1]] * x_batch.shape[0])]\n",
    "    generated_softs = []\n",
    "    while len(generated_ids) < max_decoder_steps:\n",
    "        outs, hidden_state = decoder((generated_ids[-1], thought_vectors, hidden_state))\n",
    "        generated_softs.append(outs)\n",
    "        generated_ids.append(tf.argmax(outs, -1))\n",
    "    generated_softs = np.concatenate(generated_softs, 1)[:, :y_batch.shape[1]]\n",
    "    loss = loss_fn(generated_softs, y_batch)\n",
    "    return np.asarray(generated_ids)[:, :, 0].T[:, 1:], loss\n",
    "\n",
    "def remove_symbols(ls):\n",
    "    lsnew = []\n",
    "    for i in ls:\n",
    "        if i == '$':\n",
    "            break\n",
    "        if i not in ['^', '#', '$']:\n",
    "            lsnew.append(i)\n",
    "    return lsnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "Train: epoch 1\n",
      "Epoch: [1/32], step: [1/156], loss: 2.350093\n",
      "Epoch: [1/32], step: [51/156], loss: 1.928630\n",
      "Epoch: [1/32], step: [101/156], loss: 1.898332\n",
      "Epoch: [1/32], step: [151/156], loss: 1.661202\n",
      "Test: epoch 1 loss: tf.Tensor(1.6845379, shape=(), dtype=float32)\n",
      "X: 9504+6066$\n",
      "Y: 15570$\n",
      "O: 12666$\n",
      "\n",
      "X: 9044-2933$\n",
      "Y: 6111$#\n",
      "O: 1006$$\n",
      "\n",
      "X: 7559-8128$\n",
      "Y: -569$#\n",
      "O: -100$$\n",
      "\n",
      "Train: epoch 2\n",
      "Epoch: [2/32], step: [1/156], loss: 1.678942\n",
      "Epoch: [2/32], step: [51/156], loss: 1.762639\n",
      "Epoch: [2/32], step: [101/156], loss: 1.519660\n",
      "Epoch: [2/32], step: [151/156], loss: 1.483795\n",
      "Test: epoch 2 loss: tf.Tensor(1.4485499, shape=(), dtype=float32)\n",
      "X: 3368-6131$\n",
      "Y: -2763$\n",
      "O: -2699$\n",
      "\n",
      "X: 2705+9339$\n",
      "Y: 12044$\n",
      "O: 11111$\n",
      "\n",
      "X: 8898+6002$\n",
      "Y: 14900$\n",
      "O: 14855$\n",
      "\n",
      "Train: epoch 3\n",
      "Epoch: [3/32], step: [1/156], loss: 1.504634\n",
      "Epoch: [3/32], step: [51/156], loss: 1.423163\n",
      "Epoch: [3/32], step: [101/156], loss: 1.362841\n",
      "Epoch: [3/32], step: [151/156], loss: 1.394143\n",
      "Test: epoch 3 loss: tf.Tensor(1.4476265, shape=(), dtype=float32)\n",
      "X: 5314+6059$\n",
      "Y: 11373$\n",
      "O: 11661$\n",
      "\n",
      "X: 3587+8118$\n",
      "Y: 11705$\n",
      "O: 11101$\n",
      "\n",
      "X: 4724+3727$\n",
      "Y: 8451$#\n",
      "O: 8558$$\n",
      "\n",
      "Train: epoch 4\n",
      "Epoch: [4/32], step: [1/156], loss: 1.385545\n",
      "Epoch: [4/32], step: [51/156], loss: 1.289500\n",
      "Epoch: [4/32], step: [101/156], loss: 1.286151\n",
      "Epoch: [4/32], step: [151/156], loss: 1.181127\n",
      "Test: epoch 4 loss: tf.Tensor(1.2583889, shape=(), dtype=float32)\n",
      "X: 3530+3173$\n",
      "Y: 6703$#\n",
      "O: 7222$$\n",
      "\n",
      "X: 1385+8160$\n",
      "Y: 9545$#\n",
      "O: 10132$\n",
      "\n",
      "X: 5785-7569$\n",
      "Y: -1784$\n",
      "O: -1990$\n",
      "\n",
      "Train: epoch 5\n",
      "Epoch: [5/32], step: [1/156], loss: 1.201373\n",
      "Epoch: [5/32], step: [51/156], loss: 1.105821\n",
      "Epoch: [5/32], step: [101/156], loss: 1.110499\n",
      "Epoch: [5/32], step: [151/156], loss: 0.989000\n",
      "Test: epoch 5 loss: tf.Tensor(0.99857235, shape=(), dtype=float32)\n",
      "X: 636+8686$#\n",
      "Y: 9322$#\n",
      "O: 9423$$\n",
      "\n",
      "X: 3149-2073$\n",
      "Y: 1076$#\n",
      "O: 1027$$\n",
      "\n",
      "X: 2939+6060$\n",
      "Y: 8999$#\n",
      "O: 9099$$\n",
      "\n",
      "Train: epoch 6\n",
      "Epoch: [6/32], step: [1/156], loss: 0.943339\n",
      "Epoch: [6/32], step: [51/156], loss: 0.929437\n",
      "Epoch: [6/32], step: [101/156], loss: 0.794669\n",
      "Epoch: [6/32], step: [151/156], loss: 0.713011\n",
      "Test: epoch 6 loss: tf.Tensor(0.76976895, shape=(), dtype=float32)\n",
      "X: 6836-2660$\n",
      "Y: 4176$#\n",
      "O: 4171$$\n",
      "\n",
      "X: 3730+3716$\n",
      "Y: 7446$#\n",
      "O: 7446$$\n",
      "\n",
      "X: 9813-8136$\n",
      "Y: 1677$#\n",
      "O: 1687$$\n",
      "\n",
      "Train: epoch 7\n",
      "Epoch: [7/32], step: [1/156], loss: 0.720634\n",
      "Epoch: [7/32], step: [51/156], loss: 0.750058\n",
      "Epoch: [7/32], step: [101/156], loss: 0.611805\n",
      "Epoch: [7/32], step: [151/156], loss: 0.605744\n",
      "Test: epoch 7 loss: tf.Tensor(0.63843745, shape=(), dtype=float32)\n",
      "X: 3672+5303$\n",
      "Y: 8975$#\n",
      "O: 8975$$\n",
      "\n",
      "X: 9823+6161$\n",
      "Y: 15984$\n",
      "O: 15984$\n",
      "\n",
      "X: 5134+4361$\n",
      "Y: 9495$#\n",
      "O: 9495$$\n",
      "\n",
      "Train: epoch 8\n",
      "Epoch: [8/32], step: [1/156], loss: 0.658968\n",
      "Epoch: [8/32], step: [51/156], loss: 0.474611\n",
      "Epoch: [8/32], step: [101/156], loss: 0.437949\n",
      "Epoch: [8/32], step: [151/156], loss: 0.417077\n",
      "Test: epoch 8 loss: tf.Tensor(0.46201506, shape=(), dtype=float32)\n",
      "X: 5603-6737$\n",
      "Y: -1134$\n",
      "O: -1123$\n",
      "\n",
      "X: 2996+7204$\n",
      "Y: 10200$\n",
      "O: 10100$\n",
      "\n",
      "X: 8167-6595$\n",
      "Y: 1572$#\n",
      "O: 1663$$\n",
      "\n",
      "Train: epoch 9\n",
      "Epoch: [9/32], step: [1/156], loss: 0.457585\n",
      "Epoch: [9/32], step: [51/156], loss: 0.401686\n",
      "Epoch: [9/32], step: [101/156], loss: 0.310039\n",
      "Epoch: [9/32], step: [151/156], loss: 0.309317\n",
      "Test: epoch 9 loss: tf.Tensor(0.37050036, shape=(), dtype=float32)\n",
      "X: 6484-7523$\n",
      "Y: -1039$\n",
      "O: -1039$\n",
      "\n",
      "X: 3476+9283$\n",
      "Y: 12759$\n",
      "O: 12769$\n",
      "\n",
      "X: 3514-5619$\n",
      "Y: -2105$\n",
      "O: -2106$\n",
      "\n",
      "Train: epoch 10\n",
      "Epoch: [10/32], step: [1/156], loss: 0.335309\n",
      "Epoch: [10/32], step: [51/156], loss: 0.317166\n",
      "Epoch: [10/32], step: [101/156], loss: 0.223607\n",
      "Epoch: [10/32], step: [151/156], loss: 0.287128\n",
      "Test: epoch 10 loss: tf.Tensor(0.3618673, shape=(), dtype=float32)\n",
      "X: 6029+6063$\n",
      "Y: 12092$\n",
      "O: 12092$\n",
      "\n",
      "X: 2159-6038$\n",
      "Y: -3879$\n",
      "O: -3879$\n",
      "\n",
      "X: 4656+4692$\n",
      "Y: 9348$#\n",
      "O: 9348$$\n",
      "\n",
      "Train: epoch 11\n",
      "Epoch: [11/32], step: [1/156], loss: 0.315361\n",
      "Epoch: [11/32], step: [51/156], loss: 0.202021\n",
      "Epoch: [11/32], step: [101/156], loss: 0.234931\n",
      "Epoch: [11/32], step: [151/156], loss: 0.331626\n",
      "Test: epoch 11 loss: tf.Tensor(0.43315756, shape=(), dtype=float32)\n",
      "X: 1661+5830$\n",
      "Y: 7491$#\n",
      "O: 7491$$\n",
      "\n",
      "X: 9635+8419$\n",
      "Y: 18054$\n",
      "O: 18054$\n",
      "\n",
      "X: 1649-9270$\n",
      "Y: -7621$\n",
      "O: -7621$\n",
      "\n",
      "Train: epoch 12\n",
      "Epoch: [12/32], step: [1/156], loss: 0.357126\n",
      "Epoch: [12/32], step: [51/156], loss: 0.143440\n",
      "Epoch: [12/32], step: [101/156], loss: 0.157177\n",
      "Epoch: [12/32], step: [151/156], loss: 0.196438\n",
      "Test: epoch 12 loss: tf.Tensor(0.32330456, shape=(), dtype=float32)\n",
      "X: 9967+1674$\n",
      "Y: 11641$\n",
      "O: 11651$\n",
      "\n",
      "X: 8165+2839$\n",
      "Y: 11004$\n",
      "O: 11004$\n",
      "\n",
      "X: 3431+4457$\n",
      "Y: 7888$#\n",
      "O: 7888$$\n",
      "\n",
      "Train: epoch 13\n",
      "Epoch: [13/32], step: [1/156], loss: 0.194811\n",
      "Epoch: [13/32], step: [51/156], loss: 0.245713\n",
      "Epoch: [13/32], step: [101/156], loss: 0.140518\n",
      "Epoch: [13/32], step: [151/156], loss: 0.128237\n",
      "Test: epoch 13 loss: tf.Tensor(0.12751646, shape=(), dtype=float32)\n",
      "X: 2297+5855$\n",
      "Y: 8152$#\n",
      "O: 8152$$\n",
      "\n",
      "X: 4758+3719$\n",
      "Y: 8477$#\n",
      "O: 8477$$\n",
      "\n",
      "X: 5138-6737$\n",
      "Y: -1599$\n",
      "O: -1599$\n",
      "\n",
      "Train: epoch 14\n",
      "Epoch: [14/32], step: [1/156], loss: 0.155476\n",
      "Epoch: [14/32], step: [51/156], loss: 0.123935\n",
      "Epoch: [14/32], step: [101/156], loss: 0.132599\n",
      "Epoch: [14/32], step: [151/156], loss: 0.150560\n",
      "Test: epoch 14 loss: tf.Tensor(0.21036589, shape=(), dtype=float32)\n",
      "X: 3770+2266$\n",
      "Y: 6036$#\n",
      "O: 6026$$\n",
      "\n",
      "X: 419+2919$#\n",
      "Y: 3338$#\n",
      "O: 3338$$\n",
      "\n",
      "X: 7415+7791$\n",
      "Y: 15206$\n",
      "O: 15206$\n",
      "\n",
      "Train: epoch 15\n",
      "Epoch: [15/32], step: [1/156], loss: 0.157311\n",
      "Epoch: [15/32], step: [51/156], loss: 0.133164\n",
      "Epoch: [15/32], step: [101/156], loss: 0.123526\n",
      "Epoch: [15/32], step: [151/156], loss: 0.197102\n",
      "Test: epoch 15 loss: tf.Tensor(0.16345595, shape=(), dtype=float32)\n",
      "X: 3737-7713$\n",
      "Y: -3976$\n",
      "O: -3976$\n",
      "\n",
      "X: 8094+3269$\n",
      "Y: 11363$\n",
      "O: 11363$\n",
      "\n",
      "X: 338+1107$#\n",
      "Y: 1445$#\n",
      "O: 1445$$\n",
      "\n",
      "Train: epoch 16\n",
      "Epoch: [16/32], step: [1/156], loss: 0.175047\n",
      "Epoch: [16/32], step: [51/156], loss: 0.196333\n",
      "Epoch: [16/32], step: [101/156], loss: 0.088151\n",
      "Epoch: [16/32], step: [151/156], loss: 0.083548\n",
      "Test: epoch 16 loss: tf.Tensor(0.18608262, shape=(), dtype=float32)\n",
      "X: 3473-2816$\n",
      "Y: 657$##\n",
      "O: 657$$$\n",
      "\n",
      "X: 8610+471$#\n",
      "Y: 9081$#\n",
      "O: 9970$$\n",
      "\n",
      "X: 9708-355$#\n",
      "Y: 9353$#\n",
      "O: 9313$$\n",
      "\n",
      "Train: epoch 17\n",
      "Epoch: [17/32], step: [1/156], loss: 0.117931\n",
      "Epoch: [17/32], step: [51/156], loss: 0.137362\n",
      "Epoch: [17/32], step: [101/156], loss: 0.137605\n",
      "Epoch: [17/32], step: [151/156], loss: 0.086721\n",
      "Test: epoch 17 loss: tf.Tensor(0.09650139, shape=(), dtype=float32)\n",
      "X: 4+5427$###\n",
      "Y: 5431$#\n",
      "O: 5352$$\n",
      "\n",
      "X: 6944+870$#\n",
      "Y: 7814$#\n",
      "O: 7724$$\n",
      "\n",
      "X: 5713+2691$\n",
      "Y: 8404$#\n",
      "O: 8404$$\n",
      "\n",
      "Train: epoch 18\n",
      "Epoch: [18/32], step: [1/156], loss: 0.088716\n",
      "Epoch: [18/32], step: [51/156], loss: 0.083890\n",
      "Epoch: [18/32], step: [101/156], loss: 0.083351\n",
      "Epoch: [18/32], step: [151/156], loss: 0.089669\n",
      "Test: epoch 18 loss: tf.Tensor(0.101858474, shape=(), dtype=float32)\n",
      "X: 9653-2073$\n",
      "Y: 7580$#\n",
      "O: 7570$$\n",
      "\n",
      "X: 8158+8014$\n",
      "Y: 16172$\n",
      "O: 16172$\n",
      "\n",
      "X: 5794-2518$\n",
      "Y: 3276$#\n",
      "O: 3276$$\n",
      "\n",
      "Train: epoch 19\n",
      "Epoch: [19/32], step: [1/156], loss: 0.086640\n",
      "Epoch: [19/32], step: [51/156], loss: 0.084101\n",
      "Epoch: [19/32], step: [101/156], loss: 0.066107\n",
      "Epoch: [19/32], step: [151/156], loss: 0.180417\n",
      "Test: epoch 19 loss: tf.Tensor(0.2686892, shape=(), dtype=float32)\n",
      "X: 5850-4322$\n",
      "Y: 1528$#\n",
      "O: 1518$$\n",
      "\n",
      "X: 2348+6255$\n",
      "Y: 8603$#\n",
      "O: 8603$$\n",
      "\n",
      "X: 4355+2337$\n",
      "Y: 6692$#\n",
      "O: 6692$$\n",
      "\n",
      "Train: epoch 20\n",
      "Epoch: [20/32], step: [1/156], loss: 0.152477\n",
      "Epoch: [20/32], step: [51/156], loss: 0.067133\n",
      "Epoch: [20/32], step: [101/156], loss: 0.078612\n",
      "Epoch: [20/32], step: [151/156], loss: 0.048829\n",
      "Test: epoch 20 loss: tf.Tensor(0.093161486, shape=(), dtype=float32)\n",
      "X: 8070+2413$\n",
      "Y: 10483$\n",
      "O: 10483$\n",
      "\n",
      "X: 4712+6630$\n",
      "Y: 11342$\n",
      "O: 11342$\n",
      "\n",
      "X: 527-2435$#\n",
      "Y: -1908$\n",
      "O: -1908$\n",
      "\n",
      "Train: epoch 21\n",
      "Epoch: [21/32], step: [1/156], loss: 0.081569\n",
      "Epoch: [21/32], step: [51/156], loss: 0.074497\n",
      "Epoch: [21/32], step: [101/156], loss: 0.106366\n",
      "Epoch: [21/32], step: [151/156], loss: 0.075105\n",
      "Test: epoch 21 loss: tf.Tensor(0.064507924, shape=(), dtype=float32)\n",
      "X: 4680-2627$\n",
      "Y: 2053$#\n",
      "O: 2053$$\n",
      "\n",
      "X: 6063+294$#\n",
      "Y: 6357$#\n",
      "O: 6447$$\n",
      "\n",
      "X: 7761-6725$\n",
      "Y: 1036$#\n",
      "O: 1036$$\n",
      "\n",
      "Train: epoch 22\n",
      "Epoch: [22/32], step: [1/156], loss: 0.086151\n",
      "Epoch: [22/32], step: [51/156], loss: 0.069064\n",
      "Epoch: [22/32], step: [101/156], loss: 0.052103\n",
      "Epoch: [22/32], step: [151/156], loss: 0.100322\n",
      "Test: epoch 22 loss: tf.Tensor(0.10503266, shape=(), dtype=float32)\n",
      "X: 9188-6644$\n",
      "Y: 2544$#\n",
      "O: 2544$$\n",
      "\n",
      "X: 971-1951$#\n",
      "Y: -980$#\n",
      "O: -980$$\n",
      "\n",
      "X: 4026+418$#\n",
      "Y: 4444$#\n",
      "O: 4454$$\n",
      "\n",
      "Train: epoch 23\n",
      "Epoch: [23/32], step: [1/156], loss: 0.073090\n",
      "Epoch: [23/32], step: [51/156], loss: 0.257115\n",
      "Epoch: [23/32], step: [101/156], loss: 0.171994\n",
      "Epoch: [23/32], step: [151/156], loss: 0.065867\n",
      "Test: epoch 23 loss: tf.Tensor(0.088638045, shape=(), dtype=float32)\n",
      "X: 3486-7230$\n",
      "Y: -3744$\n",
      "O: -3744$\n",
      "\n",
      "X: 6953-4879$\n",
      "Y: 2074$#\n",
      "O: 2074$$\n",
      "\n",
      "X: 1220-6471$\n",
      "Y: -5251$\n",
      "O: -5251$\n",
      "\n",
      "Train: epoch 24\n",
      "Epoch: [24/32], step: [1/156], loss: 0.042586\n",
      "Epoch: [24/32], step: [51/156], loss: 0.045922\n",
      "Epoch: [24/32], step: [101/156], loss: 0.034111\n",
      "Epoch: [24/32], step: [151/156], loss: 0.057575\n",
      "Test: epoch 24 loss: tf.Tensor(0.14625217, shape=(), dtype=float32)\n",
      "X: 4362+5067$\n",
      "Y: 9429$#\n",
      "O: 9429$$\n",
      "\n",
      "X: 9471+7824$\n",
      "Y: 17295$\n",
      "O: 17295$\n",
      "\n",
      "X: 7570+5012$\n",
      "Y: 12582$\n",
      "O: 12582$\n",
      "\n",
      "Train: epoch 25\n",
      "Epoch: [25/32], step: [1/156], loss: 0.081379\n",
      "Epoch: [25/32], step: [51/156], loss: 0.049130\n",
      "Epoch: [25/32], step: [101/156], loss: 0.059377\n",
      "Epoch: [25/32], step: [151/156], loss: 0.070080\n",
      "Test: epoch 25 loss: tf.Tensor(0.06503871, shape=(), dtype=float32)\n",
      "X: 8514-1452$\n",
      "Y: 7062$#\n",
      "O: 7062$$\n",
      "\n",
      "X: 3563-4717$\n",
      "Y: -1154$\n",
      "O: -1154$\n",
      "\n",
      "X: 5570-4473$\n",
      "Y: 1097$#\n",
      "O: 1097$$\n",
      "\n",
      "Train: epoch 26\n",
      "Epoch: [26/32], step: [1/156], loss: 0.062217\n",
      "Epoch: [26/32], step: [51/156], loss: 0.043294\n",
      "Epoch: [26/32], step: [101/156], loss: 0.042585\n",
      "Epoch: [26/32], step: [151/156], loss: 0.041329\n",
      "Test: epoch 26 loss: tf.Tensor(0.06641694, shape=(), dtype=float32)\n",
      "X: 4928-9404$\n",
      "Y: -4476$\n",
      "O: -4476$\n",
      "\n",
      "X: 2193-2939$\n",
      "Y: -746$#\n",
      "O: -746$$\n",
      "\n",
      "X: 528-7295$#\n",
      "Y: -6767$\n",
      "O: -6767$\n",
      "\n",
      "Train: epoch 27\n",
      "Epoch: [27/32], step: [1/156], loss: 0.096202\n",
      "Epoch: [27/32], step: [51/156], loss: 0.124824\n",
      "Epoch: [27/32], step: [101/156], loss: 0.052844\n",
      "Epoch: [27/32], step: [151/156], loss: 0.050364\n",
      "Test: epoch 27 loss: tf.Tensor(0.060175747, shape=(), dtype=float32)\n",
      "X: 8136+2503$\n",
      "Y: 10639$\n",
      "O: 10639$\n",
      "\n",
      "X: 5757-713$#\n",
      "Y: 5044$#\n",
      "O: 4044$$\n",
      "\n",
      "X: 754+6046$#\n",
      "Y: 6800$#\n",
      "O: 6700$$\n",
      "\n",
      "Train: epoch 28\n",
      "Epoch: [28/32], step: [1/156], loss: 0.037311\n",
      "Epoch: [28/32], step: [51/156], loss: 0.029752\n",
      "Epoch: [28/32], step: [101/156], loss: 0.026958\n",
      "Epoch: [28/32], step: [151/156], loss: 0.048775\n",
      "Test: epoch 28 loss: tf.Tensor(0.08651049, shape=(), dtype=float32)\n",
      "X: 7584-2730$\n",
      "Y: 4854$#\n",
      "O: 4854$$\n",
      "\n",
      "X: 7535+9915$\n",
      "Y: 17450$\n",
      "O: 17450$\n",
      "\n",
      "X: 5042+8817$\n",
      "Y: 13859$\n",
      "O: 13869$\n",
      "\n",
      "Train: epoch 29\n",
      "Epoch: [29/32], step: [1/156], loss: 0.047491\n",
      "Epoch: [29/32], step: [51/156], loss: 0.045896\n",
      "Epoch: [29/32], step: [101/156], loss: 0.119187\n",
      "Epoch: [29/32], step: [151/156], loss: 0.038804\n",
      "Test: epoch 29 loss: tf.Tensor(0.08805629, shape=(), dtype=float32)\n",
      "X: 229-9148$#\n",
      "Y: -8919$\n",
      "O: -8919$\n",
      "\n",
      "X: 1143+5015$\n",
      "Y: 6158$#\n",
      "O: 6158$$\n",
      "\n",
      "X: 588+6686$#\n",
      "Y: 7274$#\n",
      "O: 7274$$\n",
      "\n",
      "Train: epoch 30\n",
      "Epoch: [30/32], step: [1/156], loss: 0.034383\n",
      "Epoch: [30/32], step: [51/156], loss: 0.058663\n",
      "Epoch: [30/32], step: [101/156], loss: 0.034588\n",
      "Epoch: [30/32], step: [151/156], loss: 0.031944\n",
      "Test: epoch 30 loss: tf.Tensor(0.0790822, shape=(), dtype=float32)\n",
      "X: 3547-7546$\n",
      "Y: -3999$\n",
      "O: -3999$\n",
      "\n",
      "X: 7488-5825$\n",
      "Y: 1663$#\n",
      "O: 1663$$\n",
      "\n",
      "X: 2146-195$#\n",
      "Y: 1951$#\n",
      "O: 1851$$\n",
      "\n",
      "Train: epoch 31\n",
      "Epoch: [31/32], step: [1/156], loss: 0.029022\n",
      "Epoch: [31/32], step: [51/156], loss: 0.059116\n",
      "Epoch: [31/32], step: [101/156], loss: 0.064097\n",
      "Epoch: [31/32], step: [151/156], loss: 0.031509\n",
      "Test: epoch 31 loss: tf.Tensor(0.05619454, shape=(), dtype=float32)\n",
      "X: 8665+201$#\n",
      "Y: 8866$#\n",
      "O: 8866$$\n",
      "\n",
      "X: 1503+1300$\n",
      "Y: 2803$#\n",
      "O: 2803$$\n",
      "\n",
      "X: 1739-1568$\n",
      "Y: 171$##\n",
      "O: 171$$$\n",
      "\n",
      "Train: epoch 32\n",
      "Epoch: [32/32], step: [1/156], loss: 0.049178\n",
      "Epoch: [32/32], step: [51/156], loss: 0.032295\n",
      "Epoch: [32/32], step: [101/156], loss: 0.034827\n",
      "Epoch: [32/32], step: [151/156], loss: 0.028850\n",
      "Test: epoch 32 loss: tf.Tensor(0.09169806, shape=(), dtype=float32)\n",
      "X: 6978+8734$\n",
      "Y: 15712$\n",
      "O: 15712$\n",
      "\n",
      "X: 1280+6926$\n",
      "Y: 8206$#\n",
      "O: 8206$$\n",
      "\n",
      "X: 4270+1931$\n",
      "Y: 6201$#\n",
      "O: 6201$$\n",
      "\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_step = int(len(train_set) / batch_size)\n",
    "\n",
    "\n",
    "invalid_number_prediction_counts = []\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "print('Start training... \\n')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print('Train: epoch', epoch + 1)\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size=batch_size)):\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        # prepare the data (X_batch and Y_batch) for training\n",
    "        # using function batch_to_ids\n",
    "        ques_ids, _ = batch_to_ids(X_batch, word2id, 20)\n",
    "        ans_ids, _ = batch_to_ids(Y_batch, word2id, 20)\n",
    "        x_batch = np.asarray(ques_ids)\n",
    "        y_batch = np.asarray(ans_ids)\n",
    "        train_step(x_batch, y_batch)\n",
    "        predictions, loss = predict_with_loss(x_batch, y_batch)\n",
    "\n",
    "        if n_iter % 50 == 0:\n",
    "            print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" % (epoch + 1, n_epochs, n_iter + 1, n_step, loss))\n",
    "\n",
    "    X_sent, Y_sent = next(generate_batches(test_set, batch_size=batch_size))\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    # prepare test data (X_sent and Y_sent) for predicting\n",
    "    # quality and computing value of the loss function\n",
    "    # using function batch_to_ids\n",
    "    ques_ids, _ = batch_to_ids(X_sent, word2id, 20)\n",
    "    ans_ids, _ = batch_to_ids(Y_sent, word2id, 20)\n",
    "    x_batch = np.asarray(ques_ids)\n",
    "    y_batch = np.asarray(ans_ids)\n",
    "    predictions, loss = predict_with_loss(x_batch, y_batch)\n",
    "    print('Test: epoch', epoch + 1, 'loss:', loss, )\n",
    "    for x, y, p in list(zip(x_batch, y_batch, predictions))[:3]:\n",
    "        print('X:', ''.join(ids_to_sentence(x, id2word)))\n",
    "        print('Y:', ''.join(ids_to_sentence(y, id2word)))\n",
    "        print('O:', ''.join(ids_to_sentence(p, id2word)))\n",
    "        print('')\n",
    "\n",
    "    model_predictions = []\n",
    "    ground_truth = []\n",
    "    invalid_number_prediction_count = 0\n",
    "    # For the whole test set calculate ground-truth values (as integer numbers)\n",
    "    # and prediction values (also as integers) to calculate metrics.\n",
    "    # If generated by model number is not correct (e.g. '1-1'),\n",
    "    # increase invalid_number_prediction_count and don't append this and corresponding\n",
    "    # ground-truth value to the arrays.\n",
    "    for X_batch, Y_batch in generate_batches(test_set, batch_size=batch_size):\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        ques_ids, _ = batch_to_ids(X_batch, word2id, 20)\n",
    "        ans_ids, _ = batch_to_ids(Y_batch, word2id, 20)\n",
    "        x_batch = np.asarray(ques_ids)\n",
    "        y_batch = np.asarray(ans_ids)\n",
    "        predictions = predict(x_batch)\n",
    "        for row, rowt in zip(predictions, y_batch):\n",
    "            sent = ids_to_sentence(row, id2word)\n",
    "            sent = remove_symbols(sent)\n",
    "            sent = ''.join(sent)\n",
    "            try:\n",
    "                intval = int(sent)\n",
    "                model_predictions.append(intval)\n",
    "                ground_truth.append(int(''.join([str(x) for x in remove_symbols(ids_to_sentence(rowt, id2word))])))\n",
    "            except Exception:\n",
    "                invalid_number_prediction_count += 1\n",
    "    all_model_predictions.append(model_predictions)\n",
    "    all_ground_truth.append(ground_truth)\n",
    "    invalid_number_prediction_counts.append(invalid_number_prediction_count)\n",
    "\n",
    "print('\\n...training finished.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "Because our task is simple and the output is straight-forward, we will use [MAE](https://en.wikipedia.org/wiki/Mean_absolute_error) metric to evaluate the trained model during the epochs. Compute the value of the metric for the output from each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, MAE: 1917.066650, Invalid numbers: 0\n",
      "Epoch: 2, MAE: 621.299050, Invalid numbers: 0\n",
      "Epoch: 3, MAE: 532.709900, Invalid numbers: 0\n",
      "Epoch: 4, MAE: 308.023800, Invalid numbers: 0\n",
      "Epoch: 5, MAE: 263.563950, Invalid numbers: 0\n",
      "Epoch: 6, MAE: 172.975450, Invalid numbers: 0\n",
      "Epoch: 7, MAE: 126.614850, Invalid numbers: 0\n",
      "Epoch: 8, MAE: 114.709650, Invalid numbers: 0\n",
      "Epoch: 9, MAE: 121.010000, Invalid numbers: 0\n",
      "Epoch: 10, MAE: 115.920200, Invalid numbers: 0\n",
      "Epoch: 11, MAE: 132.741350, Invalid numbers: 0\n",
      "Epoch: 12, MAE: 104.583650, Invalid numbers: 0\n",
      "Epoch: 13, MAE: 33.266050, Invalid numbers: 0\n",
      "Epoch: 14, MAE: 52.793900, Invalid numbers: 0\n",
      "Epoch: 15, MAE: 93.571250, Invalid numbers: 0\n",
      "Epoch: 16, MAE: 35.929950, Invalid numbers: 0\n",
      "Epoch: 17, MAE: 22.361354, Invalid numbers: 3\n",
      "Epoch: 18, MAE: 25.786239, Invalid numbers: 1\n",
      "Epoch: 19, MAE: 90.178709, Invalid numbers: 1\n",
      "Epoch: 20, MAE: 25.050900, Invalid numbers: 0\n",
      "Epoch: 21, MAE: 20.665667, Invalid numbers: 2\n",
      "Epoch: 22, MAE: 33.821250, Invalid numbers: 0\n",
      "Epoch: 23, MAE: 21.633800, Invalid numbers: 0\n",
      "Epoch: 24, MAE: 34.762600, Invalid numbers: 0\n",
      "Epoch: 25, MAE: 26.622500, Invalid numbers: 0\n",
      "Epoch: 26, MAE: 22.234750, Invalid numbers: 0\n",
      "Epoch: 27, MAE: 18.824700, Invalid numbers: 0\n",
      "Epoch: 28, MAE: 19.852493, Invalid numbers: 1\n",
      "Epoch: 29, MAE: 15.605280, Invalid numbers: 1\n",
      "Epoch: 30, MAE: 25.872550, Invalid numbers: 0\n",
      "Epoch: 31, MAE: 20.763700, Invalid numbers: 0\n",
      "Epoch: 32, MAE: 24.842342, Invalid numbers: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "for i, (gts, predictions, invalid_number_prediction_count) in enumerate(zip(all_ground_truth,\n",
    "                                                                            all_model_predictions,\n",
    "                                                                            invalid_number_prediction_counts), 1):\n",
    "    mae = mean_absolute_error(gts, predictions)\n",
    "    print(\"Epoch: %i, MAE: %f, Invalid numbers: %i\" % (i, mae, invalid_number_prediction_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
